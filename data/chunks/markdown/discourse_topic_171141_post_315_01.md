---
chunk_id: discourse_topic_171141_post_315_01
source_url: https://discourse.onlinedegree.iitm.ac.in/t/171141/315
source_title: Tds-official-Project1-discrepencies
content_type: discourse
tokens: 882
username: 22f2000008
post_number: 315
topic_id: 171141
---

 A with 8/10 marks but today it is showing 0 marks on my email, but when I run it just now it is showing 4/10 on my vs code.

Whereas when I download the file from GitHub and run it, it is showing 1/10 now.

---

**[Discussion Image by 22f2000008]** The image shows a student's attempt at Task A10 of the TDS LLM Automation Agent project, indicating a peer discussion around discrepancies. The VS Code editor displays code in "app.py" and other project files. The terminal output shows that the task involves querying a SQLite database ("ticket-sales.db") for the total sales of "Gold" tickets. The error message "A10 FAILED" indicates that the student's result (200401.84) does not match the expected value (177250.79), implying the student's code or logic is incorrect, leading to a misunderstanding of the task requirements or an error in calculations.s in the \"Gold\" ticket type? Write the number in /data/ticket-sales-gold.txt". The HTTP request shows the agent attempting to read data from "/data/ticket-sales-gold.txt". The student is likely debugging discrepancies between the expected and actual outputs, representing a troubleshooting situation." alt="image" data-base62-sha1="msUKaHFancESX8hfFIglGtezbfy" width="690" height="351" srcset="**[Discussion Image by 22f2000008]** This image shows a failing test case (A10) within the "Tds-official-Project1-discrepencies" discussion, indicating a student is encountering an error. The test requires the system to calculate the total sales of "Gold" tickets from a SQLite database file named `ticket-sales.db`, storing the result in `/data/ticket-sales-gold.txt`. The expected result is `177250.79`, but the actual result from the system is `200401.84`, leading to the test failure. The terminal output shows the HTTP request made to retrieve the answer, and the "app.py" file is open in the editor, likely where the core logic resides. The screenshot is providing context of a student sharing their project results indicating that the automated test is failing and they are asking their peers or TAs about what the discrepancy is., **[Discussion Image by 22f2000008]** This image captures a student's attempt to complete Task A10 in the LLM Automation Agent project. The VS Code IDE displays several Python files in the editor, including "app.py", "datagen.py", "evaluate.py", "tasksA.py", and "tasksB.py". The terminal output shows that the A10 task involves querying a SQLite database for "Gold" ticket sales and writing the total to a file. However, the actual result (200401.84) doesn't match the expected value (177250.79), causing the A10 task to fail. This indicates a discrepancy in the calculated total sales. The student is likely troubleshooting the code logic to identify the source of the incorrect calculation. 1.5x, **[Discussion Image by 22f2000008]** This image captures a student's attempt to complete the "A10" task within the LLM Automation Agent project, showcasing a discrepancy between the expected and actual results when processing data from 'data/ticket-sales-gold.txt'. The task involves querying a SQLite database ('data/ticket-sales.db') for concert tickets of type "Gold" and calculating the total sales. The terminal output reveals that the student's implementation returned a result of 200401.84, while the expected value was 177250.79, leading to the "A10 FAILED" message and a low score of 1/10. The image shows the student's VS Code editor with open files like 'app.py', 'datagen.py', 'evaluate.py', and 'tasksA.py', as well as the terminal output, suggesting an issue with the data processing logic or database query within the 'app.py' or related files. The 'app.py' dependencies are listed as requests, fastapi, uvicorn, python-dateutil, and pandas. 2x" data-dominant-color="272727">image1897Ã—965 112 KB
