---
chunk_id: course_parsing_json_001
source_url: https://tds.s-anand.net/#/parsing-json
source_title: parsing-json
content_type: course
tokens: 550
---

 sources or services. This ensures that you can effectively process and utilize the log data generated by the web service in a structured way.)](https://youtu.be/1lxrb_ezP-g)

This requires us to handle complex nested structures, large files that don't fit in memory, or extract specific fields. Here are the key tools and techniques for efficient JSON parsing:

---

| Tool | Extract from JSON... | Why |
| ------------------------------------------- | ---------------------- | ----------------------------------------------------------------- |
| [jq](#command-line-json-processing-with-jq) | JSON in the shell | Quick data exploration and pipeline processing |
| [JMESPath](#jmespath-queries) | JSON in Python | Handle complex queries with a clean syntax |
| [ijson](#streaming-with-ijson) | JSON streams in Python | Parse streaming/large JSON files memory-efficiently |
| [Pandas](#pandas-json-columns) | JSON columns in Python | Fast analysis of structured data |
| [SQL JSON](#sql-json-functions) | JSON in databases | Combine structured and semi-structured data |
| [DuckDB](#duckdb-json-processing) | JSON anywhere | Fast analysis of JSON files / databases without loading to memory |

**Examples:**

- Use Pandas when you need to transform API responses into a DataFrame for further analysis.
- Leverage ijson when dealing with huge JSON logs where memory is at a premium.
- Apply jq for quick, iterative exploration directly in your terminal.

Practice with these resources:

- [JSONPath Online Evaluator](https://jsonpath.com/): Test JSON queries
- [jq play](https://jqplay.org/): Interactive jq query testing
- [DuckDB JSON Tutorial](https://duckdb.org/docs/data/json): Learn DuckDB JSON functions

### Command-line JSON Processing with jq

[jq](https://jqlang.org/) is a versatile command-line tool for slicing, filtering, and transforming JSON. It excels in quick data exploration and can be integrated into shell scripts for automated data pipelines.

**Example:** Sifting through server logs in JSON Lines format to extract error messages or aggregate metrics without launching a full-scale ETL process.

```bash
# Extract specific fields from JSONL
cat data.jsonl | jq -c 'select(.type == "user") | {id, name}'

# Transform JSON structure
cat data.json | jq '.items[] | {name: .name, count: .details.count}'

# Filter and aggregate
cat events.jsonl | jq -s 'group_by(.category) | map({category: .[0].category, count: length})'
```

### JMESPath Queries
