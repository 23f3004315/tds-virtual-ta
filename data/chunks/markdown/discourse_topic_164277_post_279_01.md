---
chunk_id: discourse_topic_164277_post_279_01
source_url: https://discourse.onlinedegree.iitm.ac.in/t/164277/279
source_title: Project 1 - LLM-based Automation Agent - Discussion Thread [TDS Jan 2025]
content_type: discourse
tokens: 348
username: 23f1002382
post_number: 279
topic_id: 164277
---

2159

{â€˜roleâ€™: â€˜assistantâ€™, â€˜contentâ€™: â€˜3009142952159â€™, â€˜refusalâ€™: None} if LLM is giving wrong output. I hope yâ€™all look into edge cases. Some people tried really hard. to prompt it xD .

You can check the logs

---

(venv) @ANDIECOOLER2 âœ /workspaces/TDS-Project-1/app (checking-with-open-ai) $ uv run evaluate.py 
ğŸŸ¡ Running task: Install `uv` (if required) and run the script `https://raw.githubusercontent.com/ANdIeCOOl/TDS-Project1-Ollama_FastAPI-/refs/heads/main/datagen.py`
with `23f1002382@ds.study.iitm.ac.in` as the only argument
HTTP Request: POST http://localhost:8000/run?task=
Install+`uv`+(if+required)+and+run+the+script+`https%3A%2F%2Fraw.githubusercontent.com%2FANdIeCOOl%2FTDS-Project1-Ollama_FastAPI-%2Frefs%2Fheads%2Fmain%2Fdatagen.py`
with+`23f1002382%40ds.study.iitm.ac.in`+as+the+only+argument
 â€œHTTP/1.1 200 OKâ€

HTTP 200 {

â€œstatusâ€: â€œsuccessâ€,

â€œmessageâ€: â€œTask executed successfullyâ€

}

HTTP Request: GET http://localhost:8000/read?path=/data/format.md â€œHTTP/1.1 200 OKâ€

A1 PASSED

10.8.2
