---
chunk_id: discourse_topic_163247_post_92_02
source_url: https://discourse.onlinedegree.iitm.ac.in/t/163247/92
source_title: GA3 - Large Language Models - Discussion Thread [TDS Jan 2025]
content_type: discourse
tokens: 795
username: 22f2001630
post_number: 92
topic_id: 163247
---

 up extra tokens, suggesting the token count is higher than just the visible words. Students are expected to input the number of tokens in a provided text field before checking their answer. 2x" data-dominant-color="C5D1CF">image1920×1080 248 KB

---

**[Discussion Image by 22f2001630]** This image captures a student's exploration of the OpenAI tokenizer tool in the context of a TDS (Text Data Science) course discussion, specifically related to Large Language Models. The student has navigated to platform.openai.com/tokenizer and is using the tool to understand how text is tokenized. The input text "List only the valid English words from these" results in a token count of 10 and a character count of 47, with the tool highlighting each token. The student is interacting with the "Text" view, as opposed to the "Token IDs" view. This activity is likely related to understanding how LLMs process and represent text, and how tokenization impacts model performance. for Python and `@dbdq/tiktoken` for JavaScript for programmatic tokenization." alt="image" data-base62-sha1="7T5yf8NnBgy5jKI1UqIyu9rgtFn" width="690" height="388" srcset="**[Discussion Image by 22f2001630]** This image captures a student's exploration of the OpenAI tokenizer tool, likely part of the "GA3 - Large Language Models" discussion. The student is using the tokenizer at platform.openai.com/tokenizer to analyze the input text "list only the valid English words from these". The tool displays that the input text is broken into 10 tokens and consists of 47 characters, highlighting each word with a different color to visualize the tokenization process. The screen also shows the "Clear" and "Show example" buttons along with "Text" and "Token IDs" tabs, and provides descriptive text about token lengths and links to relevant tiktoken packages for Python and Javascript. This is a practical exercise related to LLM tokenization, showing the student experimenting with the tool's interface., **[Discussion Image by 22f2001630]** This image depicts a student using the OpenAI tokenizer tool found at platform.openai.com/tokenizer, likely as part of a discussion within the "GA3 - Large Language Models" course. The student has input the text "List only the valid English words from these;" into the tokenizer. The tool indicates that this text consists of 10 tokens and 47 characters. The screenshot includes information about token estimation, explaining that one token generally corresponds to ~4 characters and provides links to tiktoken packages for Python and JavaScript for programmatic tokenization. This suggests that the student is exploring tokenization concepts within the context of LLMs. 1.5x, **[Discussion Image by 22f2001630]** This image captures a student's interaction with the OpenAI tokenizer tool, as part of the TDS Jan 2025 "GA3 - Large Language Models" discussion. The student has input the phrase "List only the valid English words from these:" into the tokenizer, which reports that the phrase consists of 10 tokens and 47 characters. The OpenAI tokenizer interface displays the "Clear" and "Show example" buttons, with tabs for "Text" and "Token IDs." The tool also provides a general guideline stating that approximately 4 characters correspond to one token in common English text, translating to roughly 100 tokens for 75 words. It recommends using the "tiktoken" package for Python and the "@dbdq/tiktoken" package for JavaScript if a programmatic tokenization interface is needed. 2x" data-dominant-color="E1E7E7">image1920×1080 225 KB
